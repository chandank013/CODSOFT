{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef54cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SPAM SMS DETECTION - EXPLORATORY DATA ANALYSIS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Spam SMS Detection - Exploratory Data Analysis\n",
    "# CodSoft ML Internship - Task 4\n",
    "# Author: Chandan Kumar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPAM SMS DETECTION - EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad476ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading dataset...\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Shape: (5572, 2)\n",
      "   Rows: 5,572\n",
      "   Columns: 2\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATASET\n",
    "\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "# Dataset typically has 'v1' (label) and 'v2' (message) columns\n",
    "df = pd.read_csv('../data/spam.csv', encoding='latin-1')\n",
    "\n",
    "# Clean columns if there are extra ones\n",
    "if 'Unnamed: 2' in df.columns:\n",
    "    df = df[['v1', 'v2']]\n",
    "    df.columns = ['label', 'message']\n",
    "else:\n",
    "    df.columns = ['label', 'message']\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40df04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA OVERVIEW\n",
      "======================================================================\n",
      "\n",
      "üìä First 10 samples:\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
      "6   ham  Even my brother is not like to speak with me. ...\n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
      "8  spam  WINNER!! As a valued network customer you have...\n",
      "9  spam  Had your mobile 11 months or more? U R entitle...\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "\n",
      "üîç Label Distribution:\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. INITIAL DATA INSPECTION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä First 10 samples:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüîç Label Distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "======================================================================\n",
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üîÑ Removing 403 duplicate messages...\n",
      "   New shape: (5169, 2)\n"
     ]
    }
   ],
   "source": [
    "# 3. MISSING VALUES CHECK\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Dropping {missing_values.sum()} missing values...\")\n",
    "    df = df.dropna()\n",
    "\n",
    "# Remove duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"\\nüîÑ Removing {duplicates} duplicate messages...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"   New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c407864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPAM vs HAM DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "üìä Message Distribution:\n",
      "   Ham (Legitimate): 4,516 (87.37%)\n",
      "   Spam: 653 (12.63%)\n",
      "\n",
      "üìà Spam Rate: 12.63%\n",
      "   Imbalance Ratio: 1:6.92\n",
      "\n",
      "‚úÖ Visualization saved: ../images/spam_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# 4. LABEL DISTRIBUTION\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPAM vs HAM DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "label_percentages = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nüìä Message Distribution:\")\n",
    "print(f\"   Ham (Legitimate): {label_counts['ham']:,} ({label_percentages['ham']:.2f}%)\")\n",
    "print(f\"   Spam: {label_counts['spam']:,} ({label_percentages['spam']:.2f}%)\")\n",
    "\n",
    "spam_ratio = label_counts['spam'] / label_counts['ham']\n",
    "print(f\"\\nüìà Spam Rate: {label_percentages['spam']:.2f}%\")\n",
    "print(f\"   Imbalance Ratio: 1:{1/spam_ratio:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(['Ham', 'Spam'], label_counts.values, \n",
    "            color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('SMS Message Distribution')\n",
    "for i, v in enumerate(label_counts.values):\n",
    "    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "axes[1].pie(label_counts.values, labels=['Ham', 'Spam'], \n",
    "            autopct='%1.2f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('SMS Message Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/spam_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Visualization saved: ../images/spam_distribution.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae531de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MESSAGE LENGTH ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìè Overall Statistics:\n",
      "   Mean length: 79 characters\n",
      "   Median length: 60 characters\n",
      "   Mean words: 15 words\n",
      "\n",
      "üìè By Category:\n",
      "\n",
      "   HAM:\n",
      "      Mean length: 70 chars\n",
      "      Median length: 52 chars\n",
      "      Mean words: 14 words\n",
      "\n",
      "   SPAM:\n",
      "      Mean length: 138 chars\n",
      "      Median length: 149 chars\n",
      "      Mean words: 24 words\n",
      "\n",
      "‚úÖ Length analysis saved: ../images/message_length_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# 5. MESSAGE LENGTH ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MESSAGE LENGTH ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate message lengths\n",
    "df['message_length'] = df['message'].apply(len)\n",
    "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nüìè Overall Statistics:\")\n",
    "print(f\"   Mean length: {df['message_length'].mean():.0f} characters\")\n",
    "print(f\"   Median length: {df['message_length'].median():.0f} characters\")\n",
    "print(f\"   Mean words: {df['word_count'].mean():.0f} words\")\n",
    "\n",
    "print(\"\\nüìè By Category:\")\n",
    "for label in ['ham', 'spam']:\n",
    "    subset = df[df['label'] == label]\n",
    "    print(f\"\\n   {label.upper()}:\")\n",
    "    print(f\"      Mean length: {subset['message_length'].mean():.0f} chars\")\n",
    "    print(f\"      Median length: {subset['message_length'].median():.0f} chars\")\n",
    "    print(f\"      Mean words: {subset['word_count'].mean():.0f} words\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length distribution\n",
    "df[df['label'] == 'ham']['message_length'].hist(bins=50, alpha=0.7, \n",
    "                                                  label='Ham', color='green', ax=axes[0])\n",
    "df[df['label'] == 'spam']['message_length'].hist(bins=50, alpha=0.7, \n",
    "                                                   label='Spam', color='red', ax=axes[0])\n",
    "axes[0].set_xlabel('Message Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Message Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 500)\n",
    "\n",
    "# Word count distribution\n",
    "df[df['label'] == 'ham']['word_count'].hist(bins=50, alpha=0.7, \n",
    "                                              label='Ham', color='green', ax=axes[1])\n",
    "df[df['label'] == 'spam']['word_count'].hist(bins=50, alpha=0.7, \n",
    "                                               label='Spam', color='red', ax=axes[1])\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Count Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/message_length_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Length analysis saved: ../images/message_length_analysis.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9682abbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEXT PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "üîÑ Applying preprocessing...\n",
      "\n",
      "üìù Sample before/after:\n",
      "\n",
      "Original: Go until jurong point, crazy.. Available only in bugis n great world la e buffet...\n",
      "Cleaned:  go until jurong point crazy available only in bugis n great world la e buffet ci...\n",
      "\n",
      "Original: Ok lar... Joking wif u oni......\n",
      "Cleaned:  ok lar joking wif u oni...\n",
      "\n",
      "Original: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 8...\n",
      "Cleaned:  free entry in a wkly comp to win fa cup final tkts st may text fa to to receive ...\n"
     ]
    }
   ],
   "source": [
    "# 6. TEXT PREPROCESSING\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\nüîÑ Applying preprocessing...\")\n",
    "df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nüìù Sample before/after:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df.iloc[i]['message'][:80]}...\")\n",
    "    print(f\"Cleaned:  {df.iloc[i]['cleaned_message'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4884856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WORD FREQUENCY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üî§ Top 10 Ham Words:\n",
      "   i: 2075\n",
      "   you: 1773\n",
      "   to: 1474\n",
      "   the: 1048\n",
      "   a: 960\n",
      "   u: 890\n",
      "   and: 818\n",
      "   in: 753\n",
      "   me: 712\n",
      "   my: 668\n",
      "\n",
      "üî§ Top 10 Spam Words:\n",
      "   to: 594\n",
      "   a: 332\n",
      "   call: 305\n",
      "   you: 259\n",
      "   your: 241\n",
      "   √•¬£: 221\n",
      "   free: 190\n",
      "   for: 184\n",
      "   the: 181\n",
      "   now: 157\n",
      "\n",
      "‚úÖ Word frequency saved: ../images/word_frequency.png\n"
     ]
    }
   ],
   "source": [
    "# 7. WORD FREQUENCY ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get words from each category\n",
    "ham_words = ' '.join(df[df['label'] == 'ham']['cleaned_message']).split()\n",
    "spam_words = ' '.join(df[df['label'] == 'spam']['cleaned_message']).split()\n",
    "\n",
    "# Most common words\n",
    "ham_common = Counter(ham_words).most_common(20)\n",
    "spam_common = Counter(spam_words).most_common(20)\n",
    "\n",
    "print(\"\\nüî§ Top 10 Ham Words:\")\n",
    "for word, count in ham_common[:10]:\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "print(\"\\nüî§ Top 10 Spam Words:\")\n",
    "for word, count in spam_common[:10]:\n",
    "    print(f\"   {word}: {count}\")\n",
    "\n",
    "# Visualization - Word frequency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ham words\n",
    "ham_words_list = [word for word, count in ham_common]\n",
    "ham_counts_list = [count for word, count in ham_common]\n",
    "axes[0].barh(ham_words_list[:15], ham_counts_list[:15], color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('Top 15 Words in Ham Messages')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Spam words\n",
    "spam_words_list = [word for word, count in spam_common]\n",
    "spam_counts_list = [count for word, count in spam_common]\n",
    "axes[1].barh(spam_words_list[:15], spam_counts_list[:15], color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 15 Words in Spam Messages')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/word_frequency.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Word frequency saved: ../images/word_frequency.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b544b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING WORD CLOUDS\n",
      "======================================================================\n",
      "\n",
      "‚òÅÔ∏è  Generating Ham word cloud...\n",
      "‚òÅÔ∏è  Generating Spam word cloud...\n",
      "‚úÖ Word clouds saved!\n"
     ]
    }
   ],
   "source": [
    "# 8. WORD CLOUDS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING WORD CLOUDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ham word cloud\n",
    "print(\"\\n‚òÅÔ∏è  Generating Ham word cloud...\")\n",
    "ham_text = ' '.join(df[df['label'] == 'ham']['cleaned_message'])\n",
    "ham_wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          colormap='Greens').generate(ham_text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(ham_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Ham Messages', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/wordcloud_ham.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Spam word cloud\n",
    "print(\"‚òÅÔ∏è  Generating Spam word cloud...\")\n",
    "spam_text = ' '.join(df[df['label'] == 'spam']['cleaned_message'])\n",
    "spam_wordcloud = WordCloud(width=800, height=400, \n",
    "                           background_color='white',\n",
    "                           colormap='Reds').generate(spam_text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(spam_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Spam Messages', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/wordcloud_spam.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"‚úÖ Word clouds saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959f4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPECIAL CHARACTERS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üîç Special Character Patterns:\n",
      "\n",
      "   has_url:\n",
      "      ham: 2 (0.0%)\n",
      "      spam: 91 (13.9%)\n",
      "\n",
      "   has_currency:\n",
      "      ham: 18 (0.4%)\n",
      "      spam: 217 (33.2%)\n",
      "\n",
      "   has_phone:\n",
      "      ham: 1 (0.0%)\n",
      "      spam: 355 (54.4%)\n"
     ]
    }
   ],
   "source": [
    "# 9. SPECIAL CHARACTERS ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPECIAL CHARACTERS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count special characters\n",
    "df['has_url'] = df['message'].apply(lambda x: 1 if 'http' in x.lower() or 'www' in x.lower() else 0)\n",
    "df['has_currency'] = df['message'].apply(lambda x: 1 if '$' in x or '¬£' in x or '‚Ç¨' in x else 0)\n",
    "df['has_phone'] = df['message'].apply(lambda x: 1 if re.search(r'\\d{10}', x) else 0)\n",
    "df['exclamation_count'] = df['message'].apply(lambda x: x.count('!'))\n",
    "df['capital_count'] = df['message'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "print(\"\\nüîç Special Character Patterns:\")\n",
    "for feature in ['has_url', 'has_currency', 'has_phone']:\n",
    "    print(f\"\\n   {feature}:\")\n",
    "    for label in ['ham', 'spam']:\n",
    "        count = df[df['label'] == label][feature].sum()\n",
    "        total = len(df[df['label'] == label])\n",
    "        print(f\"      {label}: {count} ({count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING PROCESSED DATA\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Processed data saved: data/spam_processed.csv\n",
      "‚úÖ Summary saved: ../artifacts/eda_summary.json\n"
     ]
    }
   ],
   "source": [
    "# 10. SAVE PROCESSED DATA\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Encode labels\n",
    "df['label_encoded'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Save processed data\n",
    "df.to_csv('../data/spam_processed.csv', index=False)\n",
    "print(\"\\n‚úÖ Processed data saved: ../data/spam_processed.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'total_messages': int(len(df)),\n",
    "    'ham_messages': int(label_counts['ham']),\n",
    "    'spam_messages': int(label_counts['spam']),\n",
    "    'spam_percentage': float(label_percentages['spam']),\n",
    "    'avg_message_length': float(df['message_length'].mean()),\n",
    "    'avg_word_count': float(df['word_count'].mean()),\n",
    "    'spam_avg_length': float(df[df['label'] == 'spam']['message_length'].mean()),\n",
    "    'ham_avg_length': float(df[df['label'] == 'ham']['message_length'].mean())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../artifacts/eda_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "print(\"‚úÖ Summary saved: ../artifacts/eda_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b5ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Overview:\n",
      "   - Total Messages: 5,169\n",
      "   - Ham Messages: 4,516 (87.37%)\n",
      "   - Spam Messages: 653 (12.63%)\n",
      "\n",
      "üîç Key Findings:\n",
      "\n",
      "1. CLASS IMBALANCE\n",
      "   - Moderate imbalance: 12.6% spam\n",
      "   - Ratio 1:7\n",
      "\n",
      "2. MESSAGE LENGTH\n",
      "   - Spam messages are longer: 138 chars\n",
      "   - Ham messages are shorter: 70 chars\n",
      "\n",
      "3. SPAM INDICATORS\n",
      "   - More URLs in spam messages\n",
      "   - More currency symbols ($, ¬£) in spam\n",
      "   - More phone numbers in spam\n",
      "   - More exclamation marks in spam\n",
      "\n",
      "4. COMMON SPAM WORDS\n",
      "   - free, call, prize, win, urgent\n",
      "   - txt, claim, guaranteed, cash\n",
      "\n",
      "5. COMMON HAM WORDS\n",
      "   - ok, thanks, got, will, can\n",
      "   - love, home, day, time\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   - Focus on text features (TF-IDF)\n",
      "   - Handle class imbalance (class weights)\n",
      "   - Use character/special char features\n",
      "   - Consider n-grams (unigrams + bigrams)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EXPLORATORY DATA ANALYSIS COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Generated Files:\n",
      "   ‚úÖ ../artifacts/spam_distribution.png\n",
      "   ‚úÖ ../artifacts/message_length_analysis.png\n",
      "   ‚úÖ ../artifacts/word_frequency.png\n",
      "   ‚úÖ ../artifacts/wordcloud_ham.png\n",
      "   ‚úÖ ../artifacts/wordcloud_spam.png\n",
      "   ‚úÖ ../artifacts/eda_summary.json\n",
      "   ‚úÖ ../data/spam_processed.csv\n",
      "\n",
      "üöÄ Next Steps:\n",
      "   1. Run model_training.ipynb\n",
      "   2. Use TF-IDF for feature extraction\n",
      "   3. Test multiple classifiers\n"
     ]
    }
   ],
   "source": [
    "# 11. KEY INSIGHTS\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Dataset Overview:\n",
    "   - Total Messages: {len(df):,}\n",
    "   - Ham Messages: {label_counts['ham']:,} ({label_percentages['ham']:.2f}%)\n",
    "   - Spam Messages: {label_counts['spam']:,} ({label_percentages['spam']:.2f}%)\n",
    "\n",
    "üîç Key Findings:\n",
    "\n",
    "1. CLASS IMBALANCE\n",
    "   - Moderate imbalance: {label_percentages['spam']:.1f}% spam\n",
    "   - Ratio 1:{1/spam_ratio:.0f}\n",
    "\n",
    "2. MESSAGE LENGTH\n",
    "   - Spam messages are longer: {df[df['label']=='spam']['message_length'].mean():.0f} chars\n",
    "   - Ham messages are shorter: {df[df['label']=='ham']['message_length'].mean():.0f} chars\n",
    "\n",
    "3. SPAM INDICATORS\n",
    "   - More URLs in spam messages\n",
    "   - More currency symbols ($, ¬£) in spam\n",
    "   - More phone numbers in spam\n",
    "   - More exclamation marks in spam\n",
    "\n",
    "4. COMMON SPAM WORDS\n",
    "   - free, call, prize, win, urgent\n",
    "   - txt, claim, guaranteed, cash\n",
    "\n",
    "5. COMMON HAM WORDS\n",
    "   - ok, thanks, got, will, can\n",
    "   - love, home, day, time\n",
    "\n",
    "üí° RECOMMENDATIONS:\n",
    "   - Focus on text features (TF-IDF)\n",
    "   - Handle class imbalance (class weights)\n",
    "   - Use character/special char features\n",
    "   - Consider n-grams (unigrams + bigrams)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EXPLORATORY DATA ANALYSIS COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"   ‚úÖ ../artifacts/spam_distribution.png\")\n",
    "print(\"   ‚úÖ ../artifacts/message_length_analysis.png\")\n",
    "print(\"   ‚úÖ ../artifacts/word_frequency.png\")\n",
    "print(\"   ‚úÖ ../artifacts/wordcloud_ham.png\")\n",
    "print(\"   ‚úÖ ../artifacts/wordcloud_spam.png\")\n",
    "print(\"   ‚úÖ ../artifacts/eda_summary.json\")\n",
    "print(\"   ‚úÖ ../data/spam_processed.csv\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Run model_training.ipynb\")\n",
    "print(\"   2. Use TF-IDF for feature extraction\")\n",
    "print(\"   3. Test multiple classifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codsoft_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
