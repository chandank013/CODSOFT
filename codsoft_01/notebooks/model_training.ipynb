{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8771ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MOVIE GENRE CLASSIFICATION - MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading preprocessed data...\n",
      "‚úÖ Training data shape: (43371, 5000)\n",
      "‚úÖ Validation data shape: (10843, 5000)\n",
      "‚úÖ Number of genres: 27\n"
     ]
    }
   ],
   "source": [
    "# Movie Genre Classification - Model Training with Hyperparameter Tuning\n",
    "# CodSoft ML Internship - Task 1\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, f1_score, precision_score, \n",
    "                             recall_score, make_scorer)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MOVIE GENRE CLASSIFICATION - MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"\\nüìÇ Loading preprocessed data...\")\n",
    "X_train = np.load('../artifacts/X_train_tfidf.npy')\n",
    "X_val = np.load('../artifacts/X_val_tfidf.npy')\n",
    "y_train = np.load('../artifacts/y_train.npy')\n",
    "y_val = np.load('../artifacts/y_val.npy')\n",
    "\n",
    "with open('../artifacts/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Training data shape: {X_train.shape}\")\n",
    "print(f\"‚úÖ Validation data shape: {X_val.shape}\")\n",
    "print(f\"‚úÖ Number of genres: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4cf9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: BASELINE MODELS (Without Hyperparameter Tuning)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# BASELINE MODELS \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: BASELINE MODELS (Without Hyperparameter Tuning)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def train_baseline_model(model, model_name):\n",
    "    \"\"\"Train baseline model without hyperparameter tuning\"\"\"\n",
    "    print(f\"\\nüîÑ Training {model_name} (Baseline)...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[f\"{model_name} (Baseline)\"] = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'model_type': 'baseline'\n",
    "    }\n",
    "    \n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eaadb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Training Logistic Regression (Baseline)...\n",
      "  Accuracy:  0.5771\n",
      "  Precision: 0.5561\n",
      "  Recall:    0.5771\n",
      "  F1-Score:  0.5358\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic Regression Baseline\n",
    "lr_baseline, lr_pred_baseline = train_baseline_model(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Logistic Regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e42bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Training Naive Bayes (Baseline)...\n",
      "  Accuracy:  0.5239\n",
      "  Precision: 0.5087\n",
      "  Recall:    0.5239\n",
      "  F1-Score:  0.4464\n"
     ]
    }
   ],
   "source": [
    "# 2. Naive Bayes Baseline\n",
    "nb_baseline, nb_pred_baseline = train_baseline_model(\n",
    "    MultinomialNB(),\n",
    "    \"Naive Bayes\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240d0ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Training Linear SVM (Baseline)...\n",
      "  Accuracy:  0.5653\n",
      "  Precision: 0.5355\n",
      "  Recall:    0.5653\n",
      "  F1-Score:  0.5416\n"
     ]
    }
   ],
   "source": [
    "# 3. Linear SVM Baseline\n",
    "svm_baseline, svm_pred_baseline = train_baseline_model(\n",
    "    LinearSVC(random_state=42, max_iter=1000),\n",
    "    \"Linear SVM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b79386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: HYPERPARAMETER TUNING (Finding Best Parameters)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define scoring metric\n",
    "scoring = make_scorer(f1_score, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOGISTIC REGRESSION - Grid Search\n",
    "\n",
    "print(\"\\nüîç Tuning Logistic Regression...\")\n",
    "print(\"   Testing different regularization strengths and solvers...\")\n",
    "\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    lr_param_grid,\n",
    "    cv=3,  # Reduced from 5 to save memory\n",
    "    scoring=scoring,\n",
    "    n_jobs=2,  # Limited parallel jobs to avoid memory issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   Running Grid Search (this may take a few minutes)...\")\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Parameters: {lr_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {lr_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ab70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Logistic Regression\n",
    "lr_best = lr_grid.best_estimator_\n",
    "y_pred_lr = lr_best.predict(X_val)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_val, y_pred_lr)\n",
    "lr_precision = precision_score(y_val, y_pred_lr, average='weighted')\n",
    "lr_recall = recall_score(y_val, y_pred_lr, average='weighted')\n",
    "lr_f1 = f1_score(y_val, y_pred_lr, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä Validation Results:\")\n",
    "print(f\"   Accuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"   Precision: {lr_precision:.4f}\")\n",
    "print(f\"   Recall:    {lr_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {lr_f1:.4f}\")\n",
    "\n",
    "results[\"Logistic Regression (Tuned)\"] = {\n",
    "    'accuracy': float(lr_accuracy),\n",
    "    'precision': float(lr_precision),\n",
    "    'recall': float(lr_recall),\n",
    "    'f1_score': float(lr_f1),\n",
    "    'best_params': lr_grid.best_params_,\n",
    "    'cv_score': float(lr_grid.best_score_),\n",
    "    'model_type': 'tuned'\n",
    "}\n",
    "\n",
    "all_models['Logistic Regression'] = lr_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. NAIVE BAYES - Grid Search\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"üîç Tuning Naive Bayes...\")\n",
    "print(\"   Testing different smoothing parameters...\")\n",
    "\n",
    "nb_param_grid = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "nb_grid = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    nb_param_grid,\n",
    "    cv=3,  # Reduced from 5 to save memory\n",
    "    scoring=scoring,\n",
    "    n_jobs=2,  # Limited parallel jobs to avoid memory issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   Running Grid Search...\")\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Parameters: {nb_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {nb_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Naive Bayes\n",
    "nb_best = nb_grid.best_estimator_\n",
    "y_pred_nb = nb_best.predict(X_val)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_val, y_pred_nb)\n",
    "nb_precision = precision_score(y_val, y_pred_nb, average='weighted')\n",
    "nb_recall = recall_score(y_val, y_pred_nb, average='weighted')\n",
    "nb_f1 = f1_score(y_val, y_pred_nb, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä Validation Results:\")\n",
    "print(f\"   Accuracy:  {nb_accuracy:.4f}\")\n",
    "print(f\"   Precision: {nb_precision:.4f}\")\n",
    "print(f\"   Recall:    {nb_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {nb_f1:.4f}\")\n",
    "\n",
    "results[\"Naive Bayes (Tuned)\"] = {\n",
    "    'accuracy': float(nb_accuracy),\n",
    "    'precision': float(nb_precision),\n",
    "    'recall': float(nb_recall),\n",
    "    'f1_score': float(nb_f1),\n",
    "    'best_params': nb_grid.best_params_,\n",
    "    'cv_score': float(nb_grid.best_score_),\n",
    "    'model_type': 'tuned'\n",
    "}\n",
    "\n",
    "all_models['Naive Bayes'] = nb_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LINEAR SVM - Grid Search\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"üîç Tuning Linear SVM...\")\n",
    "print(\"   Testing different regularization parameters...\")\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    LinearSVC(random_state=42),\n",
    "    svm_param_grid,\n",
    "    cv=3,  # Reduced from 5 to save memory\n",
    "    scoring=scoring,\n",
    "    n_jobs=2,  # Limited parallel jobs to avoid memory issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   Running Grid Search...\")\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Parameters: {svm_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {svm_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best SVM\n",
    "svm_best = svm_grid.best_estimator_\n",
    "y_pred_svm = svm_best.predict(X_val)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_val, y_pred_svm)\n",
    "svm_precision = precision_score(y_val, y_pred_svm, average='weighted')\n",
    "svm_recall = recall_score(y_val, y_pred_svm, average='weighted')\n",
    "svm_f1 = f1_score(y_val, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä Validation Results:\")\n",
    "print(f\"   Accuracy:  {svm_accuracy:.4f}\")\n",
    "print(f\"   Precision: {svm_precision:.4f}\")\n",
    "print(f\"   Recall:    {svm_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {svm_f1:.4f}\")\n",
    "\n",
    "results[\"Linear SVM (Tuned)\"] = {\n",
    "    'accuracy': float(svm_accuracy),\n",
    "    'precision': float(svm_precision),\n",
    "    'recall': float(svm_recall),\n",
    "    'f1_score': float(svm_f1),\n",
    "    'best_params': svm_grid.best_params_,\n",
    "    'cv_score': float(svm_grid.best_score_),\n",
    "    'model_type': 'tuned'\n",
    "}\n",
    "\n",
    "all_models['Linear SVM'] = svm_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RANDOM FOREST - Grid Search \n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"üîç Tuning Random Forest (Bonus Model)...\")\n",
    "print(\"   Testing different tree configurations...\")\n",
    "print(\"   Note: Using smaller parameter grid to avoid memory issues\")\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    cv=3,  # Reduced from 5 to save memory\n",
    "    scoring=scoring,\n",
    "    n_jobs=2,  # Limited parallel jobs to avoid memory issues\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"   Running Grid Search...\")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Parameters: {rf_grid.best_params_}\")\n",
    "print(f\"‚úÖ Best CV Score: {rf_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccedb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Random Forest\n",
    "rf_best = rf_grid.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_val)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_val, y_pred_rf)\n",
    "rf_precision = precision_score(y_val, y_pred_rf, average='weighted')\n",
    "rf_recall = recall_score(y_val, y_pred_rf, average='weighted')\n",
    "rf_f1 = f1_score(y_val, y_pred_rf, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä Validation Results:\")\n",
    "print(f\"   Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"   Precision: {rf_precision:.4f}\")\n",
    "print(f\"   Recall:    {rf_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {rf_f1:.4f}\")\n",
    "\n",
    "results[\"Random Forest (Tuned)\"] = {\n",
    "    'accuracy': float(rf_accuracy),\n",
    "    'precision': float(rf_precision),\n",
    "    'recall': float(rf_recall),\n",
    "    'f1_score': float(rf_f1),\n",
    "    'best_params': rf_grid.best_params_,\n",
    "    'cv_score': float(rf_grid.best_score_),\n",
    "    'model_type': 'tuned'\n",
    "}\n",
    "\n",
    "all_models['Random Forest'] = rf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ab724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON & SELECTION\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: MODEL COMPARISON & BEST MODEL SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare tuned models\n",
    "tuned_results = {k: v for k, v in results.items() if v['model_type'] == 'tuned'}\n",
    "\n",
    "print(\"\\nüìä Tuned Models Performance:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Model':<30} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, metrics in tuned_results.items():\n",
    "    print(f\"{model_name:<30} {metrics['accuracy']:<12.4f} {metrics['precision']:<12.4f} \"\n",
    "          f\"{metrics['recall']:<12.4f} {metrics['f1_score']:<12.4f}\")\n",
    "\n",
    "# Select best model based on F1-score (better for multi-class)\n",
    "best_model_name = max(tuned_results, key=lambda x: tuned_results[x]['f1_score'])\n",
    "best_model_base = best_model_name.replace(\" (Tuned)\", \"\")\n",
    "best_model = all_models[best_model_base]\n",
    "best_metrics = tuned_results[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ BEST MODEL SELECTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Accuracy:  {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_metrics['f1_score']:.4f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_metrics['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE BEST MODEL\n",
    "\n",
    "print(\"\\nüíæ Saving best model...\")\n",
    "with open('../models/model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(\"‚úÖ Best model saved: ../models/model.pkl\")\n",
    "\n",
    "# Save all models (optional)\n",
    "print(\"\\nüíæ Saving all trained models...\")\n",
    "for name, model in all_models.items():\n",
    "    filename = f\"../models/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"‚úÖ {name} saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAILED EVALUATION OF BEST MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED EVALUATION OF BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_base == 'Logistic Regression':\n",
    "    best_pred = y_pred_lr\n",
    "elif best_model_base == 'Naive Bayes':\n",
    "    best_pred = y_pred_nb\n",
    "elif best_model_base == 'Linear SVM':\n",
    "    best_pred = y_pred_svm\n",
    "else:\n",
    "    best_pred = y_pred_rf\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(y_val, best_pred, \n",
    "                               target_names=label_encoder.classes_,\n",
    "                               digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed report\n",
    "with open('artifacts/classification_report.txt', 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"MOVIE GENRE CLASSIFICATION - MODEL PERFORMANCE\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
    "    f.write(f\"Overall Accuracy: {best_metrics['accuracy']:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {best_metrics['f1_score']:.4f}\\n\\n\")\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for param, value in best_metrics['best_params'].items():\n",
    "        f.write(f\"  {param}: {value}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"All Models Comparison:\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    for model_name, metrics in results.items():\n",
    "        f.write(f\"\\n{model_name}:\\n\")\n",
    "        f.write(f\"  Accuracy:  {metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Precision: {metrics['precision']:.4f}\\n\")\n",
    "        f.write(f\"  Recall:    {metrics['recall']:.4f}\\n\")\n",
    "        f.write(f\"  F1-Score:  {metrics['f1_score']:.4f}\\n\")\n",
    "        if 'best_params' in metrics:\n",
    "            f.write(f\"  Best Parameters: {metrics['best_params']}\\n\")\n",
    "\n",
    "print(\"‚úÖ Detailed report saved: ../artifacts/classification_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ffda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATIONS\n",
    "\n",
    "print(\"\\nüìä Creating visualizations...\")\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_val, best_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - {best_model_name}\\nAccuracy: {best_metrics[\"accuracy\"]:.4f}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Confusion matrix saved: ../artifacts/confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Model Comparison - Tuned Models\n",
    "plt.figure(figsize=(12, 6))\n",
    "tuned_names = list(tuned_results.keys())\n",
    "tuned_accuracies = [tuned_results[m]['accuracy'] for m in tuned_names]\n",
    "tuned_f1_scores = [tuned_results[m]['f1_score'] for m in tuned_names]\n",
    "\n",
    "x = np.arange(len(tuned_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x - width/2, tuned_accuracies, width, label='Accuracy', alpha=0.8)\n",
    "bars2 = plt.bar(x + width/2, tuned_f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Tuned Models Performance Comparison')\n",
    "plt.xticks(x, [name.replace(' (Tuned)', '') for name in tuned_names], rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Model comparison saved: ../artifacts/model_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3. Baseline vs Tuned Comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "all_model_types = ['Logistic Regression', 'Naive Bayes', 'Linear SVM']\n",
    "baseline_scores = []\n",
    "tuned_scores = []\n",
    "\n",
    "for model_type in all_model_types:\n",
    "    baseline_key = f\"{model_type} (Baseline)\"\n",
    "    tuned_key = f\"{model_type} (Tuned)\"\n",
    "    \n",
    "    baseline_scores.append(results[baseline_key]['f1_score'])\n",
    "    tuned_scores.append(results[tuned_key]['f1_score'])\n",
    "\n",
    "x = np.arange(len(all_model_types))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x - width/2, baseline_scores, width, label='Baseline', alpha=0.8, color='lightcoral')\n",
    "bars2 = plt.bar(x + width/2, tuned_scores, width, label='Tuned', alpha=0.8, color='lightgreen')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Baseline vs Hyperparameter Tuned Models')\n",
    "plt.xticks(x, all_model_types, rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels and improvement percentages\n",
    "for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "    h1 = bar1.get_height()\n",
    "    h2 = bar2.get_height()\n",
    "    improvement = ((h2 - h1) / h1) * 100\n",
    "    \n",
    "    plt.text(bar1.get_x() + bar1.get_width()/2., h1,\n",
    "            f'{h1:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.text(bar2.get_x() + bar2.get_width()/2., h2,\n",
    "            f'{h2:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Show improvement\n",
    "    mid_x = x[i]\n",
    "    mid_y = (h1 + h2) / 2\n",
    "    plt.text(mid_x, mid_y, f'+{improvement:.1f}%', \n",
    "            ha='center', va='center', fontsize=8, \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../artifacts/baseline_vs_tuned.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Baseline vs Tuned comparison saved: ../artifacts/baseline_vs_tuned.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE METRICS AND PARAMETERS\n",
    "\n",
    "print(\"\\nüíæ Saving metrics and parameters...\")\n",
    "\n",
    "# Save all results\n",
    "with open('../artifacts/metrics.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'best_model': best_model_name,\n",
    "        'best_model_type': best_model_base,\n",
    "        'all_results': results,\n",
    "        'best_hyperparameters': best_metrics['best_params']\n",
    "    }, f, indent=4)\n",
    "print(\"‚úÖ Metrics saved: ../artifacts/metrics.json\")\n",
    "\n",
    "# Save training parameters\n",
    "params = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_model_type': best_model_base,\n",
    "    'num_features': X_train.shape[1],\n",
    "    'num_classes': len(label_encoder.classes_),\n",
    "    'training_samples': int(len(y_train)),\n",
    "    'validation_samples': int(len(y_val)),\n",
    "    'classes': label_encoder.classes_.tolist(),\n",
    "    'hyperparameter_tuning': 'enabled',\n",
    "    'cv_folds': 3,  # 3-fold CV for memory efficiency\n",
    "    'parallel_jobs': 2  # Limited to avoid memory issues\n",
    "}\n",
    "\n",
    "with open('../artifacts/params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "print(\"‚úÖ Parameters saved: ../artifacts/params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcd0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"   ‚úÖ models/model.pkl (Best model)\")\n",
    "for name in all_models.keys():\n",
    "    filename = f\"{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    print(f\"   ‚úÖ models/{filename}\")\n",
    "print(\"   ‚úÖ ../artifacts/classification_report.txt\")\n",
    "print(\"   ‚úÖ ../artifacts/confusion_matrix.png\")\n",
    "print(\"   ‚úÖ ../artifacts/model_comparison.png\")\n",
    "print(\"   ‚úÖ ../artifacts/baseline_vs_tuned.png\")\n",
    "print(\"   ‚úÖ ../artifacts/metrics.json\")\n",
    "print(\"   ‚úÖ ../artifacts/params.json\")\n",
    "\n",
    "print(\"\\nüìä Best Model Summary:\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {best_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Run experiments.ipynb to test the model\")\n",
    "print(\"   2. Use app.py for interactive predictions\")\n",
    "print(\"   3. Check artifacts/ folder for visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codsoft_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
